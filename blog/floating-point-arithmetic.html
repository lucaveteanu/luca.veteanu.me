<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Why 0.1 + 0.2 is not 0.3 in JavaScript? - Luca Veteanu</title>
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <div class="nav-brand">
                <a href="../index.html">Luca Veteanu</a>
            </div>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../about.html">About</a></li>
                <li><a href="../projects.html">Projects</a></li>
                <li><a href="../volunteering.html">Volunteering</a></li>
                <li><a href="../awards.html">Awards</a></li>
                <li><a href="../blog.html" class="active">Blog</a></li>
                <li><a href="../contact.html">Contact</a></li>
            </ul>
        </div>
    </nav>

    <section class="content-section">
        <div class="container">
            <div class="blog-post">
                <h1>Why 0.1 + 0.2 is not 0.3 in JavaScript?</h1>
                <div class="blog-post-meta">
                    <span class="blog-category">JavaScript</span> | September 2024
                </div>
                <div class="blog-post-content">
                    <p>
                        One of the strangest quirks in JavaScript (and many other programming languages) is this: if you 
                        type <code>0.1 + 0.2</code> in the console, you don't get <code>0.3</code>. Instead, you get 
                        <code>0.30000000000000004</code>. Wait, what?
                    </p>

                    <h2>The Confusion</h2>
                    <p>
                        When students first encounter this in my class, they think JavaScript is broken. "How can a 
                        computer not do basic math correctly?" It's a valid question, and the answer reveals something 
                        fundamental about how computers work.
                    </p>

                    <h2>Binary Fractions</h2>
                    <p>
                        Remember that computers think in binary - 1s and 0s. While it's easy to represent whole numbers 
                        in binary (5 is 101, 10 is 1010), decimal fractions are trickier.
                    </p>
                    <p>
                        In our decimal system, we can precisely represent 1/2 (0.5), 1/4 (0.25), and 1/8 (0.125) because 
                        these are fractions with denominators that are powers of 2. But what about 1/10?
                    </p>
                    <p>
                        Just like 1/3 becomes 0.333... (repeating forever) in decimal, 1/10 becomes a repeating fraction 
                        in binary. The computer has to round it off somewhere, and that's where the tiny error creeps in.
                    </p>

                    <h2>The Technical Details</h2>
                    <p>
                        JavaScript uses a format called IEEE 754 double-precision floating-point to store numbers. This 
                        format allocates 64 bits to store a number, with specific bits for the sign, exponent, and mantissa.
                    </p>
                    <p>
                        When you write 0.1, JavaScript converts it to the closest binary representation it can. This 
                        representation is very close to 0.1, but not exactly 0.1. The same happens with 0.2. When you 
                        add these two approximate values, the tiny errors combine, giving you 0.30000000000000004.
                    </p>

                    <h2>Is This a Problem?</h2>
                    <p>
                        For most everyday programming, these tiny errors don't matter. If you're building a game or a 
                        website, the difference between 0.3 and 0.30000000000000004 is negligible.
                    </p>
                    <p>
                        However, if you're working with money or other situations where precision is critical, you need 
                        to be careful. Never use floating-point numbers for financial calculations!
                    </p>

                    <h2>How to Handle It</h2>
                    <p>
                        There are several ways to deal with floating-point precision issues:
                    </p>
                    <ul>
                        <li><strong>Rounding:</strong> Use <code>toFixed()</code> to round to a specific number of 
                        decimal places</li>
                        <li><strong>Integer math:</strong> For money, work in cents (integers) instead of dollars 
                        (decimals)</li>
                        <li><strong>Comparison tolerance:</strong> Instead of checking if two numbers are exactly equal, 
                        check if they're close enough</li>
                        <li><strong>Special libraries:</strong> Use libraries designed for precise decimal math when 
                        needed</li>
                    </ul>

                    <h2>A Practical Example</h2>
                    <p>
                        Here's how you might safely compare floating-point numbers:
                    </p>
                    <pre><code>// Don't do this:
if (0.1 + 0.2 === 0.3) {
    console.log("Equal!");
}

// Do this instead:
function areClose(a, b, tolerance = 0.0001) {
    return Math.abs(a - b) < tolerance;
}

if (areClose(0.1 + 0.2, 0.3)) {
    console.log("Close enough!");
}</code></pre>

                    <h2>It's Not Just JavaScript</h2>
                    <p>
                        This isn't a JavaScript problem - it's how computers handle decimal numbers. Python, Java, C++, 
                        and almost every programming language has this same behavior. It's a fundamental limitation of 
                        binary floating-point arithmetic.
                    </p>

                    <h2>The Takeaway</h2>
                    <p>
                        Understanding why 0.1 + 0.2 doesn't equal exactly 0.3 teaches us something important: computers 
                        are incredibly powerful, but they're not magic. They have limitations, and good programmers know 
                        how to work within those limitations.
                    </p>
                    <p>
                        When I teach this concept, I use it as an opportunity to discuss how computers represent data at 
                        a fundamental level. It's not just a quirk to memorize - it's a window into understanding how 
                        your code really runs.
                    </p>
                    <p>
                        So the next time you see <code>0.30000000000000004</code> in your console, don't panic. 
                        JavaScript isn't broken. It's just being honest about the limitations of binary floating-point 
                        arithmetic!
                    </p>
                </div>

                <div style="margin-top: 3rem; padding-top: 2rem; border-top: 2px solid var(--border-color);">
                    <a href="../blog.html" class="btn btn-secondary">‚Üê Back to Blog</a>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 Luca Veteanu. All rights reserved.</p>
            <p>Cooper City, Florida | <a href="mailto:hello@luca.veteanu.me">hello@luca.veteanu.me</a></p>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
